{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogon01/42robotics/blob/master/notebooks/trabalhoV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6DWBROmLm96",
        "outputId": "e6ec9398-2b2a-41af-bd71-3aaf4600be2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'artificial-neural-network'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 112 (delta 48), reused 27 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (112/112), 901.24 KiB | 4.98 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "datasets  notebooks  README.md\tsrc  teste.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Clonar o repositório do GitHub no ambiente do Colab\n",
        "!git clone --branch feature/entrega-final https://github.com/felipeabe/artificial-neural-network.git\n",
        "\n",
        "# Adicionar o diretório src ao sys.path para permitir a importação de módulos\n",
        "import sys\n",
        "sys.path.append('/content/artificial-neural-network/src')\n",
        "\n",
        "# Verificar se o repositório foi clonado corretamente\n",
        "!ls /content/artificial-neural-network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T-fLx-D1L8VX"
      },
      "outputs": [],
      "source": [
        "# Importando bibliotecas necessárias\n",
        "import pandas as pd\n",
        "\n",
        "# Importando o dataset 'obesity.csv'\n",
        "# Este conjunto contém dados sobre obesidade com fatores como estilo de vida e transporte\n",
        "dados_obesidade = pd.read_csv(\"/content/artificial-neural-network/datasets/obesity.csv\")\n",
        "\n",
        "# Renomear colunas para nomes mais intuitivos\n",
        "mapear_colunas = {\n",
        "    'family_history_with_overweight': 'historico_familiar',\n",
        "    'FAVC': 'comida_calorica',\n",
        "    'FCVC': 'vegetais',\n",
        "    'NCP': 'refeicoes_dia',\n",
        "    'SMOKE': 'fuma',\n",
        "    'CH2O': 'agua_dia',\n",
        "    'SCC': 'calorias',\n",
        "    'FAF': 'atividade_fisica',\n",
        "    'CALC': 'alcool',\n",
        "    'MTRANS': 'transporte',\n",
        "    'NObeyesdad': 'nivel_obesidade'\n",
        "}\n",
        "dados_obesidade = dados_obesidade.rename(columns=mapear_colunas)\n",
        "\n",
        "# Remover colunas irrelevantes para a análise\n",
        "colunas_remover = ['CAEC', 'TUE']\n",
        "dados_obesidade = dados_obesidade.drop(columns=colunas_remover)\n",
        "\n",
        "# Codificar variáveis categóricas (binárias e escalares)\n",
        "mapeamento_binario = {\n",
        "    'Gender': {\"Female\": 0, \"Male\": 1},\n",
        "    'historico_familiar': {\"yes\": 1, \"no\": 0},\n",
        "    'comida_calorica': {\"yes\": 1, \"no\": 0},\n",
        "    'fuma': {\"yes\": 1, \"no\": 0},\n",
        "    'calorias': {\"yes\": 1, \"no\": 0}\n",
        "}\n",
        "for coluna, mapeamento in mapeamento_binario.items():\n",
        "    dados_obesidade[coluna] = dados_obesidade[coluna].map(mapeamento)\n",
        "\n",
        "# Codificar consumo de álcool e transporte\n",
        "mapeamento_alcool = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}\n",
        "dados_obesidade['alcool'] = dados_obesidade['alcool'].map(mapeamento_alcool)\n",
        "\n",
        "mapeamento_transporte = {\n",
        "    'Walking': 0, 'Bike': 0,\n",
        "    'Motorbike': 1, 'Automobile': 1,\n",
        "    'Public_Transportation': 1\n",
        "}\n",
        "dados_obesidade['transporte'] = dados_obesidade['transporte'].map(mapeamento_transporte)\n",
        "\n",
        "# Normalizar dados contínuos (altura, peso, idade)\n",
        "colunas_normalizar = ['Height', 'Weight', 'Age']\n",
        "dados_obesidade[colunas_normalizar] = dados_obesidade[colunas_normalizar].apply(\n",
        "    lambda x: ((x - x.min()) / (x.max() - x.min())).round(3)\n",
        ")\n",
        "\n",
        "# Dividir o dataset em treino (80%) e teste (20%)\n",
        "split_point = int(0.8 * len(dados_obesidade))\n",
        "treino_multiclasse = dados_obesidade[:split_point]\n",
        "teste_multiclasse = dados_obesidade[split_point:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pChH_5lqR-eP",
        "outputId": "c9b3af81-224d-49b6-8148-08bac1be8da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   price  bedrooms  grade  has_basement  living_in_m2  nice_view  \\\n",
            "0  0.215         2      2             1         0.261          0   \n",
            "1  0.213         2      2             0         0.166          0   \n",
            "2  0.223         2      2             0         0.201          1   \n",
            "3  0.171         2      3             0         0.299          0   \n",
            "4  0.576         3      2             1         0.856          0   \n",
            "5  0.339         1      2             1         0.103          1   \n",
            "6  0.149         2      2             1         0.253          0   \n",
            "7  0.143         1      1             0         0.264          0   \n",
            "8  0.116         2      2             0         0.166          0   \n",
            "9  0.432         2      3             0         0.204          0   \n",
            "\n",
            "   perfect_condition  real_bathrooms  has_lavatory  single_floor  month  \\\n",
            "0                  0               2             1             0      5   \n",
            "1                  0               1             1             0     11   \n",
            "2                  0               1             0             1     12   \n",
            "3                  0               2             1             1      2   \n",
            "4                  0               3             0             0      1   \n",
            "5                  0               1             0             1      3   \n",
            "6                  0               1             1             1      6   \n",
            "7                  0               1             0             1     10   \n",
            "8                  0               1             1             1      7   \n",
            "9                  0               2             1             0      9   \n",
            "\n",
            "   ano_construcao  \n",
            "0            2014  \n",
            "1            2014  \n",
            "2            2014  \n",
            "3            2015  \n",
            "4            2015  \n",
            "5            2015  \n",
            "6            2014  \n",
            "7            2014  \n",
            "8            2014  \n",
            "9            2014  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1a7a20d52bb9>:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  dados_imoveis = dados_imoveis.replace({True: 1, False: 0})\n"
          ]
        }
      ],
      "source": [
        "# Importando bibliotecas necessárias\n",
        "import pandas as pd\n",
        "\n",
        "# Importar o dataset 'houses.csv'\n",
        "# Este conjunto contém dados sobre propriedades como preço e área útil\n",
        "dados_imoveis = pd.read_csv(\"/content/artificial-neural-network/datasets/houses.csv\")\n",
        "\n",
        "# Remover colunas irrelevantes\n",
        "# As colunas 'renovated' (renovação) e 'quartile_zone' (zona por quartil) são removidas por serem irrelevantes\n",
        "colunas_remover = ['renovated', 'quartile_zone']\n",
        "dados_imoveis = dados_imoveis.drop(columns=colunas_remover)\n",
        "\n",
        "# Codificar valores booleanos (True/False) em numéricos (1/0)\n",
        "# Transformação necessária para modelos de aprendizado que lidam com dados numéricos\n",
        "dados_imoveis = dados_imoveis.replace({True: 1, False: 0})\n",
        "\n",
        "# Extrair o ano da coluna de data\n",
        "# Converte a coluna 'date' para o formato datetime e extrai apenas o ano para análise\n",
        "dados_imoveis['date'] = pd.to_datetime(dados_imoveis['date'])\n",
        "dados_imoveis['ano_construcao'] = dados_imoveis['date'].dt.year\n",
        "dados_imoveis = dados_imoveis.drop(columns=['date'])  # Remove a coluna original de data\n",
        "\n",
        "# Normalizar colunas contínuas ('price' e 'living_in_m2') para valores entre 0 e 1\n",
        "colunas_normalizar = ['price', 'living_in_m2']\n",
        "dados_imoveis[colunas_normalizar] = dados_imoveis[colunas_normalizar].apply(\n",
        "    lambda x: ((x - x.min()) / (x.max() - x.min())).round(3)\n",
        ")\n",
        "\n",
        "# Dividir o conjunto de dados em treino (80%) e teste (20%)\n",
        "split_point = int(0.8 * len(dados_imoveis))\n",
        "treino_regressao = dados_imoveis[:split_point]  # Dados para treino\n",
        "teste_regressao = dados_imoveis[split_point:]  # Dados para teste\n",
        "\n",
        "# Exibir as 10 primeiras linhas do conjunto de treino para validação\n",
        "print(treino_regressao.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqeegzxe1c7f",
        "outputId": "2f00107b-a978-441e-d3d9-2f7ecbea62d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PatientID  Age  Gender  Ethnicity  EducationLevel        BMI  Smoking  \\\n",
            "0       4751   73       0          0               2  22.927749        0   \n",
            "1       4752   89       0          0               0  26.827681        0   \n",
            "2       4753   73       0          3               1  17.795882        0   \n",
            "3       4754   74       1          0               1  33.800817        1   \n",
            "4       4755   89       0          0               0  20.716974        0   \n",
            "\n",
            "   AlcoholConsumption  PhysicalActivity  DietQuality  ...  MemoryComplaints  \\\n",
            "0           13.297218          6.327112     1.347214  ...                 0   \n",
            "1            4.542524          7.619885     0.518767  ...                 0   \n",
            "2           19.555085          7.844988     1.826335  ...                 0   \n",
            "3           12.209266          8.428001     7.435604  ...                 0   \n",
            "4           18.454356          6.310461     0.795498  ...                 0   \n",
            "\n",
            "   BehavioralProblems       ADL  Confusion  Disorientation  \\\n",
            "0                   0  1.725883          0               0   \n",
            "1                   0  2.592424          0               0   \n",
            "2                   0  7.119548          0               1   \n",
            "3                   1  6.481226          0               0   \n",
            "4                   0  0.014691          0               0   \n",
            "\n",
            "   PersonalityChanges  DifficultyCompletingTasks  Forgetfulness  Diagnosis  \\\n",
            "0                   0                          1              0          0   \n",
            "1                   0                          0              1          0   \n",
            "2                   0                          1              0          0   \n",
            "3                   0                          0              0          0   \n",
            "4                   1                          1              0          0   \n",
            "\n",
            "   DoctorInCharge  \n",
            "0       XXXConfid  \n",
            "1       XXXConfid  \n",
            "2       XXXConfid  \n",
            "3       XXXConfid  \n",
            "4       XXXConfid  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Conjunto de treino: (1719, 36)\n",
            "Conjunto de teste: (430, 36)\n"
          ]
        }
      ],
      "source": [
        "# Importar bibliotecas necessárias\n",
        "# Utilizamos pandas para manipulação de dados e numpy para operações matemáticas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Importar o dataset 'alzheimer.csv'\n",
        "# Este conjunto contém informações sobre pacientes e diagnósticos\n",
        "dados_alzheimer = pd.read_csv(\"/content/artificial-neural-network/datasets/alzheimer.csv\")\n",
        "\n",
        "# Visualizar as primeiras linhas do conjunto de dados\n",
        "# Isso ajuda a verificar se o dataset foi carregado corretamente\n",
        "print(dados_alzheimer.head())\n",
        "\n",
        "# Remover colunas irrelevantes\n",
        "# As colunas 'PatientID' e 'DoctorInCharge' são consideradas irrelevantes para o diagnóstico\n",
        "dados_alzheimer = dados_alzheimer.drop(columns=[\"PatientID\", \"DoctorInCharge\"])\n",
        "\n",
        "# Separar variáveis categóricas e numéricas\n",
        "# Variáveis categóricas como gênero, etnia e nível educacional são processadas separadamente\n",
        "variaveis_categoricas = [\"Gender\", \"Ethnicity\", \"EducationLevel\"]\n",
        "variaveis_numericas = [col for col in dados_alzheimer.columns if col not in variaveis_categoricas + [\"Diagnosis\"]]\n",
        "\n",
        "# Codificar variáveis categóricas\n",
        "# OneHotEncoder é usado para transformar categorias em representações numéricas binárias\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "codificador = OneHotEncoder(drop=\"first\", sparse_output=False)  # Evitar multicolinearidade\n",
        "categorias_codificadas = codificador.fit_transform(dados_alzheimer[variaveis_categoricas])\n",
        "categorias_codificadas_df = pd.DataFrame(categorias_codificadas, columns=codificador.get_feature_names_out(variaveis_categoricas))\n",
        "\n",
        "# Normalizar variáveis numéricas\n",
        "# As variáveis numéricas são escaladas para manter uma escala consistente (média 0, desvio padrão 1)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "escalador = StandardScaler()\n",
        "numericas_normalizadas = escalador.fit_transform(dados_alzheimer[variaveis_numericas])\n",
        "numericas_normalizadas_df = pd.DataFrame(numericas_normalizadas, columns=variaveis_numericas)\n",
        "\n",
        "# Combinar dados processados com a variável alvo\n",
        "# As variáveis categóricas codificadas e as numéricas normalizadas são combinadas em um único conjunto\n",
        "dados_processados = pd.concat([numericas_normalizadas_df, categorias_codificadas_df], axis=1)\n",
        "dados_processados[\"Diagnosis\"] = dados_alzheimer[\"Diagnosis\"]\n",
        "\n",
        "# Dividir os dados em conjuntos de treino e teste\n",
        "# Usamos 80% dos dados para treino e 20% para teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = dados_processados.drop(columns=[\"Diagnosis\"])  # Dados de entrada\n",
        "y = dados_processados[\"Diagnosis\"]  # Variável alvo (diagnóstico)\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Exibir as dimensões dos conjuntos de treino e teste\n",
        "print(f\"Conjunto de treino: {X_treino.shape}\")\n",
        "print(f\"Conjunto de teste: {X_teste.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar o diretório `src` ao caminho do Python\n",
        "import sys\n",
        "sys.path.append('/content/artificial-neural-network/src')\n",
        "\n",
        "# Importar a implementação da Rede Neural\n",
        "from neural_network import NeuralNetwork\n",
        "from activations import relu, sigmoid, tanh\n",
        "from losses import mse_loss, cross_entropy_loss\n",
        "\n",
        "# Configurar a Rede Neural\n",
        "configuracao = {\n",
        "    \"input_size\": X_treino.shape[1],  # Número de características (colunas) no conjunto de treino\n",
        "    \"hidden_size\": 10,  # Número de neurônios na camada oculta\n",
        "    \"output_size\": 1,  # Saída com 1 neurônio (para classificação binária)\n",
        "    \"activation\": \"relu\",  # Função de ativação escolhida\n",
        "    \"loss\": \"mse\",  # Função de perda escolhida\n",
        "    \"learning_rate\": 0.01,  # Taxa de aprendizado para o gradiente descendente\n",
        "    \"epochs\": 1000  # Número de épocas de treinamento\n",
        "}\n",
        "\n",
        "# Instanciar a Rede Neural\n",
        "rede_neural = NeuralNetwork(\n",
        "    input_size=configuracao[\"input_size\"],\n",
        "    hidden_size=configuracao[\"hidden_size\"],\n",
        "    output_size=configuracao[\"output_size\"],  # Adicionado output_size\n",
        "    activation=configuracao[\"activation\"],\n",
        "    loss=configuracao[\"loss\"]\n",
        ")\n",
        "\n",
        "# Treinar a Rede Neural com os dados de treino\n",
        "# Verificar se o método correto é 'fit', 'train' ou outro equivalente\n",
        "rede_neural.fit(  # Substitua 'fit' pelo método correto implementado na classe\n",
        "    X_treino.to_numpy(),\n",
        "    y_treino.to_numpy().reshape(-1, 1),\n",
        "    configuracao[\"learning_rate\"],\n",
        "    configuracao[\"epochs\"]\n",
        ")\n",
        "\n",
        "# Avaliar a Rede Neural com os dados de teste\n",
        "acuracia = rede_neural.evaluate(  # Substitua 'evaluate' pelo método correto implementado na classe\n",
        "    X_teste.to_numpy(),\n",
        "    y_teste.to_numpy().reshape(-1, 1)\n",
        ")\n",
        "\n",
        "# Exibir o resultado da acurácia\n",
        "print(f\"Acurácia do modelo: {acuracia * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "EgypESWghqHt",
        "outputId": "004bcb23-4801-493c-a547-6eb5da6f6a66"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'float' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ce3b1dedef16>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Treinar a Rede Neural com os dados de treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Verificar se o método correto é 'fit', 'train' ou outro equivalente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m rede_neural.fit(  # Substitua 'fit' pelo método correto implementado na classe\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mX_treino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0my_treino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/artificial-neural-network/src/neural_network.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}